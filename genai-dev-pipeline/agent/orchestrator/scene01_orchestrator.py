# agent/orchestrator/scene01_orchestrator.py

from agent.schema.scene01.request import Scene01Request, ChatTurn
from agent.core.llm_client import LLMClient
from agent.core.retriever_client import RetrieverClient
from agent.schema.scene01.response import Scene01Response, GeneratedMessage
from typing import List
import logging

logger = logging.getLogger(__name__)

class Scene01Orchestrator:
    def __init__(self):
        self.llm_client = LLMClient()
        self.retriever_client = RetrieverClient()

    async def run(self, request: Scene01Request) -> Scene01Response:
        # 1. íˆìŠ¤í† ë¦¬ ì •ë¦¬ (ìµœê·¼ 3ê°œ ë¬¸ë‹µ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        history_blocks = self._format_history(request.history)

        # 2. ê¸°íšì•ˆ ìš”ì•½ (ì„ íƒ)
        file_summary = await self._summarize_uploaded_file(request.file_url) if request.file_url else ""

        # 3. í”„ë¡¬í”„íŠ¸ ê²°í•©
        final_prompt = self._compose_final_prompt(
            history_blocks,
            request.prompt,
            request.life_stage,
            file_summary
        )

        # 4. RAG ìˆ˜í–‰
        rag_augmented_knowledge = await self._retrieve_knowledge_with_rag(
            message_type=request.message_type.value,
            life_stage=request.life_stage,
            channels=[c.value for c in request.channels],
            query=final_prompt
        )

        # 5. ì±„ë„ë³„ ë©”ì‹œì§€ ìƒì„±
        generated_messages: List[GeneratedMessage] = []
        for channel in request.channels:
            prompt_per_channel = self._build_prompt_per_channel(
                base_prompt=final_prompt,
                rag_knowledge=rag_augmented_knowledge,
                channel=channel.value
            )
            response_text = await self.llm_client.generate(
                llm_id=999,  # ğŸ§  LLM IDëŠ” configë¡œ ë¶„ë¦¬ ê°€ëŠ¥
                messages=[prompt_per_channel],
                llm_config={"temperature": 0.7, "maxTokens": 512}
            )
            generated_messages.append(GeneratedMessage(channel=channel.value, content=response_text.strip()))

        # 6. í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ ì‘ë‹µ
        result = "\n".join([f"{m.channel}: {m.content}" for m in generated_messages])

        return Scene01Response(
            result=result,
            used_prompt=final_prompt
        )

    # ========================= ë‚´ë¶€ í•¨ìˆ˜ ì •ì˜ =========================

    def _format_history(self, history: List[ChatTurn]) -> str:
        if not history:
            return ""
        return "\n".join(
            [f"User: {turn.user}\nAI: {turn.ai}" for turn in history]
        )

    async def _summarize_uploaded_file(self, file_url: str) -> str:
        # TODO: PDF, DOCX, PPTX ìš”ì•½ ë¡œì§ êµ¬í˜„ ì˜ˆì • (LangChain Document Loader ê°€ëŠ¥)
        logger.info(f"Summarizing uploaded file: {file_url}")
        return f"[ê¸°íšì•ˆ ìš”ì•½]: (ìš”ì•½ëœ ë‚´ìš©ì´ ì—¬ê¸°ì— ë“¤ì–´ê°‘ë‹ˆë‹¤)"

    def _compose_final_prompt(self, history: str, user_prompt: str, life_stage: str, file_summary: str) -> str:
        return f"""[íˆìŠ¤í† ë¦¬]
{history}

[ë¼ì´í”„ìŠ¤í…Œì´ì§€]
{life_stage} ëŒ€ìƒ ê³ ê°ì—ê²Œ ì í•©í•œ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

[ì‚¬ìš©ì ìš”ì²­]
{user_prompt}

{file_summary}
"""

    async def _retrieve_knowledge_with_rag(self, message_type: str, life_stage: str, channels: List[str], query: str) -> str:
        # message_typeì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ RAG Agent ID ì‚¬ìš© ê°€ëŠ¥
        agent_id = self._select_agent_id_by_message_type(message_type)
        rag_response = await self.retriever_client.retrieve_and_generate(
            agent_id=agent_id,
            query=query,
            llm_config={"temperature": 0.3, "maxTokens": 512}
        )
        return rag_response.strip()

    def _select_agent_id_by_message_type(self, message_type: str) -> int:
        # TODO: ì‹¤ì œ Agent ID ë§¤í•‘ í…Œì´ë¸” ì—°ë™
        mapping = {
            "ëŒ€ê³ ê°ë©”ì‹œì§€": 101,
            "ê´‘ê³ ì‹¬ì˜ë¬¸êµ¬": 202,
            "ìœ ì˜ë¬¸êµ¬": 203
        }
        return mapping.get(message_type, 101)

    def _build_prompt_per_channel(self, base_prompt: str, rag_knowledge: str, channel: str) -> str:
        return f"""{base_prompt}

[ì¶”ê°€ ì •ë³´ (RAG ê¸°ë°˜)]:
{rag_knowledge}

[ì¶œë ¥ì¡°ê±´]
ì´ ì±„ë„({channel})ì— ì í•©í•œ ê¸¸ì´ë¡œ ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
